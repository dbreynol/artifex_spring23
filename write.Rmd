# Other Topics

## Data Processing

To motivate this section on data processing, we will use the (fictional) data below that contains order information for a retail company. As is almost always the case, this data needs some pre-processing before it is in shape for exploration and analysis. Copy and paste the code below to obtain this data frame in your environment. The processing steps we carry out below rely on the <code>lubridate</code> package, so start your script with <code>library(lubridate)</code>.

```{r echo=TRUE}
set.seed(1)
customer = c("113 - Shaws", "217 - Shaws", "2114 - WalMart", "99 - WalMart",
             "23 - CVS", "09 - CVS") # numbers dash name
product = c("WX1X - 9 gal - Jelly", "WX1P - 4 gal - Jelly",
            "QP1X - 11 gal - Grape Juice", "QP1 - 7 gal - Fruit Juice",
            "TYL - 1 gal - Peanut Butter", "LL - 2 gal - Jam") # letters size description
business_unit = c("123 Retail", "437 Consumer",
                  "990 International", "222 Retail",
                  "49 Consumer", "09 International") # number dash name

df = data.frame(customer, product, business_unit, 
                "1-1-2022" = rpois(6, 1100),
                "2-1-2022" = rpois(6, 1200),
                check.names = F)
```

```{r}
kable(head(df),
      caption = 'A fake dataset on product shipments.')
```

Our pre-processing of this particular dataset consists primarily of cleaning up the character (string) variables and dealing with dates.  

### Cleaning up strings

For a reference on the use of strings in R, see this [excellent resource](https://r4ds.had.co.nz/strings.html#strings).

For the purpose of our analysis, some aspects of this dataset that are distracting for analysis include:

1. The meaningless (to us) numbers that precede the customer name and business unit. 
2. The meaningless letters that precede the product description.
3. The product variable includes the size which would ideally exist in its own column (i.e., we want a size column whose first entry is 9)

Let's work through each of these using the functions <code>str_sub</code>, <code>str_locate</code>, and <code>str_split</code>. Let's first go over what these functions do and how they can be used for this task.

```{r echo=TRUE, eval=FALSE}
str_sub("The big short", start = 5, end = 7) # big
# the input can also be a vector
x <- c("Apple", "Banana", "Pear") # character vector
str_sub(x, 1, 3) # "App" "Ban" "Pea"
```

```{r echo=TRUE, eval=FALSE}
str_locate("This", "s") # gives the starting and ending position of the 's' in 'This'
```

**How can these two functions** (<code>str_sub</code>, <code>str_locate</code>) **be used in conjunction to deal with the first issue identified above? Try this out using the first entry from customer.**

The patterns that are located can be much more general than a literal letter. These patters are known as regular expressions. For example, suppose we would like to locate the part of the string that consists of the letter a followed by any number. The regular expression that represents any digit is <code>\\d</code>.

```{r echo=TRUE, eval=FALSE}
str_locate("aardvark a3", "a\\d") # 10 11
```

For more on matching regular expressions in R, [see this document on the topic](https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions). 

Next, let's deal with the product variable. This variable contains two potentially useful pieces of information and one useless piece. They are separated by a dash. This presents a good use case for the <code>str_split</code> function. Here is an example where we have two data points that are separated by a colon and we would like to extract them.

```{r echo=TRUE, eval=FALSE}
str_split("200:300", ":", simplify = T) # 200 300
```

**What should we split the product variable on?**  

This will give us a matrix, with two useful columns. We want to grab these columns and include them in our data frame as <code>size</code> and <code>product_name</code>. [Refresher on how to carry out these sorts of tasks - How R thinks about data.](http://htmlpreview.github.io/?https://github.com/andrewpbray/oiLabs-base-R/blob/master/intro_to_data/intro_to_data.html)

### Assignment 1

1. Use the functions described above (<code>str_sub</code>, <code>str_locate</code>, and <code>str_split</code>) to obtain the following cleaned up data frame.

```{r}

df2 = df %>% mutate(customer = str_sub(customer, start = str_locate(customer, "-")[,1] + 2)) %>%
  mutate(size = str_split(product, " - ", simplify = T)[,2]) %>%
  mutate(product = str_split(product, " - ", simplify = T)[,3]) %>%
  mutate(business_unit = str_sub(business_unit, start = str_locate(business_unit, "\\d+ ")[,2] + 1)) #%>%
  #select(-product)

kable(head(df2))

```

2. **Dealing with dates (part 1)**. The output from part one gets us pretty close to our desired analytic dataset. However, it turns out that for our analysis we would like the data to be in *long* format, where each row contains information on a single month. So, the first row of the dataset would have the January 2022 units of 9 gal Jelly shipped to Shaw's. This is achieved using the function <code>pivot_longer</code>. [Let's take a look at the example from the documentation](https://tidyr.tidyverse.org/reference/pivot_longer.html) to figure out what parameters to use to achieve this

Once you figure out the appropriate arguments for <code>pivot_longer</code>, the <code>head</code> of your new data frame should look like this:

```{r}
# pivot_longer(df2, cols = c(`1-1-2022`,`2-1-2022`), names_to = "month", values_to = "units")
df3 = pivot_longer(df2, cols = matches("\\d"), names_to = "month", values_to = "units")
kable(head(df3))
```

3. **Dealing with dates (part 2)**. Finally, we want R to recognize our <code>month</code> variable as a date (rather than a character). A helpful package for dates is <code>lubridate</code>. Download and load this package into your workspace and use the function <code>mdy</code> on the <code>month</code> variable to coerce this variable to become a date. The <code>str</code> of your data frame should now look like this:

```{r}
# pivot_longer(df2, cols = c(`1-1-2022`,`2-1-2022`), names_to = "month", values_to = "units")
df3 = mutate(df3, month = mdy(month))
kable(str(df3))
```

Notice how the month variable has format *Date*. Your data frame is now ready for exploration and analysis!

## Time Series

### Decomposition

We want to decompose our time series into three parts: a trend component ($T$), a seasonality component ($S$), and a random component ($R$). That is, for each observation $Y_t$, we want to break it down into three parts:

$Y_T = T_t + S_t + R_t$

First, let's talk about the trend component (T). The trend component is a moving average, which you can obtain using the <code>ma</code> function within the *forecast* library.

```{r echo=TRUE}
set.seed(1)

# generate some fake data that resembles our real dataset (noce the week_no variable, which 
# corresponds with the enterprise week number from your original data)

trend = seq( from = 10, to = 10 + 52 * 3 - 1, by = 1)

df = data.frame( invoiced = trend + (rnorm(52 * 3, mean = 25, sd = 40)), 
                 week_no = rep(1:52, 3), # week number
                 id = 1:(52 * 3)) 
plot(df$invoiced, type = "l")

y_ts = ts(df$invoiced, frequency = 52) # make your data into a time series object
df$trend = ma(y_ts, order = 52) # use the ma function from the forecast library
```

Now that we have added a trend variable to our data frame, let's get the seasonal ($S$) component. To estimate the seasonal component for each week, simply average the detrended values for that week. These seasonal component values are then adjusted to ensure that they add to zero.

```{r, echo=T}
# 1. subtract the trend
df$detrend = df$invoiced - df$trend

# 2. group by week number and take the average of the de-trended values
df = df %>% group_by(week_no) %>% 
  mutate(S1 = mean(detrend, na.rm = T)) %>% 
  ungroup() %>% 
  mutate(S = S1 - mean(S1)) %>% # make sure the sum of the seasonal components is 0
  select(!c(detrend, S1))
```

Finally, the random component is calculated by subtracting the estimated seasonal and trend-cycle components. That is, 

$R_t = Y_t - T_t - S_t$.

```{r, echo=TRUE}
df = mutate(df, R = invoiced - trend - S)
```

Now, plot each line:

```{r echo=TRUE, warning=FALSE, message=FALSE}
df %>% 
  pivot_longer(!(c("week_no", "id"))) %>% 
  ggplot(aes(id, value, color = name)) + 
  geom_line() + 
  theme_minimal() + 
  ggtitle("Decomposition of invoiced time series")
```

### Forecasting

Let's consider some very simple potential forecast models. The simplest potential model is to forecast future values as the last observed value. That is,

\begin{equation}
\hat{y}_{T+1 | T} = y_T
\end{equation}

Another very simple model consists of forecasting future values as the average over the entire observed series. That is,

\begin{equation}
\hat{y}_{T+1 | T} = \frac{1}{T} \sum_{i=1}^T y_i
\end{equation}

A slightly more complicated approach that typically works better is to forecast future values as a weighted average of past values, with higher weights assigned to more recent observations. This model can be expressed as,

\begin{equation}
\hat{y}_{T+h | T} = \alpha y_T + (1-\alpha) \hat{y}_{T | T - 1}
\end{equation}

How can we show that weights are decreasing in time? What does this expression imply about forecasts from this model? This is implemented in <code>ses</code> below.

The next level up in complexity is using the same idea but adding a trend estimate. In this case, the forecast values are,

\begin{equation}
\hat{y}_{T+h | T} = l_T + h b_T,
\end{equation}

Where $l$ is the estimated level of the time series and $b_T$ is the estimated trend at time $T$. What does this expression imply about forecasts from this model? This is implemented in <code>holt</code> below.

```{r echo=TRUE}

fx1 <- ses(y_ts, h=5)
fx2 <- holt(y_ts, h=15)

round(accuracy(fx1),2)

autoplot(y_ts) +
  autolayer(fx1, series="Simple Exponential Smoothing", PI=F) +
  autolayer(fx2, series="Holt Winters", PI=F) + 
  ylab("Invoiced") + xlab("Year")
```


### Statistical model

One of the drawbacks of the forecasting methods we have used so far is that there is no uncertainty quantification. Fortunately, there is an easy way to formulate the exponential smoothing models we have been working with as a statistical model. For more detail, [see this excellent resource](https://otexts.com/fpp2/ets.html).



```{r echo=TRUE, warning=FALSE}
mod = ets(y_ts, model="ZZZ", damped=NULL, alpha=NULL, beta=NULL,
    gamma=NULL, phi=NULL, lambda=NULL, biasadj=FALSE,
    additive.only=FALSE, restrict=TRUE,
    allow.multiplicative.trend=FALSE)
```

Once the model is fit, we can see which model was chosen using the <code>summary</code> function. We label each state space model as ETS( $\cdot, \cdot, \cdot$), for (Error, Trend, Seasonal). The possibilities for each component are: Error = {$A,M$}, Trend = {$N,A,A_d$}, and Seasonal = {$N,A,M$}. Here, $M$ denotes multiply and $A$ denotes add. In an $M$ type model, the component multiplies (i.e., a season in which invoices are 1.1 times as large).

```{r}
summary(mod)
```

There is also a straightforward way to forecast using the fitted model.

```{r echo=TRUE}
mod %>% forecast(h=15) %>% autoplot()
```

**To do**

1. Cut your time series into a training set (all but the last 3 months of data) and a test set (last 3 months)
2. Fit a state-space model using <code>ETS</code> to your (weekly) time series. What model was chosen? Compare forecasts for 12 weeks ahead to the actual data.
3. Now make your training set and test set into monthly data and repeat step 2.
4. On a monthly basis, how do the residuals of the weekly model compare to the residuals of the monthly model?







