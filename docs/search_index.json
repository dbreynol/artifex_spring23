[["index.html", "Artifex - Spring 2023 Course Overview - Artifex Spring 2023 Prerequisites and Preparations References", " Artifex - Spring 2023 David Reynolds 2023-02-28 Course Overview - Artifex Spring 2023 Welcome (back) to Artifex! The primary objective of this course/ club is to equip its members with the essential skills of a data scientist. This is primarily achieved by working through a real world business analytics project with an industry partner. The course delivery is a mix of lectures and project-based learning. The lectures will all be tailored to our project and will be more prevalent in the beginning of the course. All lecture materials will be posted on this website. Further details on the partner project will be posted on Canvas. This class also has a significant club component, which mostly means that there is an emphasis on collaboration and getting to know one another. Prerequisites and Preparations You should have some basic knowledge of R, and be familiar with the topics covered in the Chapters 1 and 2 here. Have a recent version of R and RStudio installed. Install and load the tidyverse package. install.packages(&quot;tidyverse&quot;) library(tidyverse) References Grolemund, G &amp; Wickham, H (2017): R for Data Science http://r4ds.had.co.nz Hyndman, R &amp; Athanasopoulos, G (2022): Forecasting: Principles and Practice https://otexts.com/fpp2/index.html "],["eda-1-traditional-dfs.html", "Chapter 1 EDA 1 (traditional df’s) 1.1 From base R to ggplot2 1.2 Linear regression 1.3 Assignment 1", " Chapter 1 EDA 1 (traditional df’s) 1.1 From base R to ggplot2 R has several systems for making graphs, but ggplot2 is one of the most versatile. Using ggplot2 requires that you have loaded the tidyverse package. I’ll use the dataset msleep to show some key concepts. Let’s use our first graph to answer a question: what is the relationship between REM sleep and total sleep? These are both numerical variables, so a good plot choice is a scatterplot. Using R’s built in plot function: plot(msleep$sleep_total, msleep$sleep_rem) How can we do the same thing in ggplot2? ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) Let’s go through each of these things. First, DATA is the name of your data frame (or tibble). Second, GEOM_FUNCTION specifies the type of plot; ggplot2 comes with many geom functions that each add a different type of layer to a plot. Each geom function in ggplot2 takes a MAPPING argument. This defines how variables in your dataset are mapped to visual properties. The mapping argument is always paired with aes(), and the x and y arguments of aes() specify which variables to map to the x and y axes; ggplot2 looks for the mapped variables in the data argument, in this case, msleep. For a scatterplot, the GEOM_FUNCTION is geom_point. So, we can generate our scatterplot as, ggplot(data = msleep) + geom_point(mapping = aes(x = sleep_total, y = sleep_rem)) We might ask whether this relationship depends on the mammal’s diet. We could approach this question by color coding the points based on what the mammal eats, as specified by the vore variable. ggplot(data = msleep) + geom_point(mapping = aes(x = sleep_total, y = sleep_rem, color = vore)) Another way, particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data. To facet your plot by a single variable, use facet_wrap(). The first argument of facet_wrap() should be a formula, which you create with ~ followed by a variable name (here “formula” is the name of a data structure in R, not a synonym for “equation”). The variable that you pass to facet_wrap() should be discrete. ggplot(data = msleep) + geom_point(mapping = aes(x = sleep_total, y = sleep_rem)) + facet_wrap( ~ vore, nrow = 2) From these plots, it seems that there is a linear relationship between total sleep and REM sleep for herbivores and carnivores but not necessarily for the other mammal types. We might further ask whether this relationship depends on the mammal’s diet and body size. We could approach this question by further size coding the points based on size, as specified by the bodywt variable. ggplot(data = filter(msleep, bodywt &lt;= 1000)) + geom_point(mapping = aes(x = sleep_total, y = sleep_rem, color = vore, size = (bodywt))) It looks like the bigger mammals tend to get little sleep and the smaller animals get more sleep, on average. 1.2 Linear regression We can take our EDA one step further by fitting a linear regression to the data. In the last section, we noticed that there is a linear relationship between rem sleep and total sleep for herbivores and carnivores. To learn more about this relationship, we can subset our data to just these two vore types and fit a linear regression model to the data. This model will have the form: \\[\\begin{equation} \\text{sleep_rem} = \\hat{\\beta_0} + \\hat{\\beta_1} \\times \\text{sleep_total}. \\end{equation}\\] Before we fit the model, let us subset the data to just the relevant observations. mod_df = filter(msleep, vore %in% c(&quot;carni&quot;, &quot;herbi&quot;)) mod = lm(sleep_rem ~ sleep_total, data = mod_df) mod %&gt;% tidy() %&gt;% kable() term estimate std.error statistic p.value (Intercept) -0.4975947 0.2831096 -1.757605 0.0883788 sleep_total 0.2298045 0.0272162 8.443680 0.0000000 How would you interpret these estimates? We could visualize this relationship by adding another GEOM_FUNCTION. mod_df %&gt;% ggplot(aes(x = sleep_total, y = sleep_rem)) + geom_point() + geom_smooth(formula = y ~ x, method = lm) Another question we could address is whether this relationship is different for herbivores and carnivores. This can easily be addressed visually by making the linetype depend on vore. mod_df %&gt;% ggplot(aes(x = sleep_total, y = sleep_rem)) + geom_point(aes(color = vore)) + geom_smooth(formula = y ~ x, method = lm, aes(linetype = vore, color = vore)) + theme_minimal() 1.3 Assignment 1 For this assignment, you will investigate the relationship between sleep_rem (explanatory variable) and the brain’s proportion of total bodyweight (i.e., brainwt/bodywt - this is the response variable). To do this, do the following: Modify your data frame to include the response variable. Name this variable brain_proportion. Hint: use the mutate function. Remove any observations that have an NA value for either the response or explanatory variable. Hint: use the drop_na function. Visualize the relationship between the explanatory and response variable. Does the relationship change based on vore? If so, how? Remove any vore types that have fewer than 5 observations and again visualize the relationship between the explanatory and response variable. Is the relationship different between vore types? Are there any outliers? msleep %&gt;% mutate(brain_proportion= brainwt/bodywt) %&gt;% group_by(vore) %&gt;% mutate(n = n()) %&gt;% filter(n &gt; 5) %&gt;% drop_na(brain_proportion, sleep_rem) %&gt;% ggplot(aes(x = sleep_rem, y = brain_proportion, color = vore)) + geom_point() + geom_smooth(aes(linetyp = vore), method = lm) "],["eda-2---time-series.html", "Chapter 2 EDA 2 - Time Series 2.1 Coercing time variables to dates 2.2 Assignment 2", " Chapter 2 EDA 2 - Time Series 2.1 Coercing time variables to dates We are going to look at storms. To motivate this discussion, let’s explore the distribution of storm duration by status (hurricane, tropical depression, tropical storm). To get duration, we should first make sure we have a date variable that we can use. We will do this using the lubridate package and the function mdy. storms2 = storms %&gt;% # first concatenate the month, day, year -&gt; then coerce it to a date using mdy mutate(date_mdy = mdy( str_c(month, day, year, sep = &quot;/&quot;)) ) Now, lets get the duration for each storm by name and status. This new variable will be a data structure called difftime. storms3 = storms2 %&gt;% group_by(name, year, status) %&gt;% summarise(dur = max(date_mdy) - min(date_mdy), .groups = &quot;drop&quot;) Finally, let’s visualize the distribution for each storm type. ggplot(storms3, aes(x = as.numeric(dur, unit = &quot;weeks&quot;))) + geom_histogram(bins = 50) + facet_wrap( ~ status) + xlab(&quot;Duration (in weeks)&quot;) This is a little hard to differentiate. Let’s plot them all on the same plot instead. ggplot(storms3, aes(x = as.numeric(dur, unit = &quot;weeks&quot;), color = status)) + geom_density(aes(linetype = status)) 2.2 Assignment 2 How many storm names are there? What year had the largest number of hurricanes? What is the median number of tropical storms per year? What year had the fewest number of tropical depressions? How has the number of storms (by each type) per year changed over time? Use a plot to answer this question using geom_line. How has the number of category 5 hurricanes per year changed over time? Use a plot to answer this question using geom_line. "],["eda-3---spatial-data.html", "Chapter 3 EDA 3 - Spatial Data 3.1 Joins and sf objects 3.2 Plotting spatial data 3.3 Assignment 3 3.4 ACS Data 3.5 Assignment 4", " Chapter 3 EDA 3 - Spatial Data 3.1 Joins and sf objects Spatial data typically consists of 2 parts: a normal old data frame and a geometry for each observation. These two parts are often contained in separate data files. So, the first topic of this section will be joining together different sets of data. We will look at the spatial distribution of crime in the US by joisning together data from the USArrests dataset with data from the us_states dataset, which is contained in the package spData. The object we will use to contain variable data as well as geographic data is an sf object, from a package with the same name. USArrests$name = rownames(USArrests) # add the state names as a variable joined_df = left_join(USArrests, us_states, by = c(&quot;name&quot; = &quot;NAME&quot;)) # join the arrests and states geo data joined_df_sf = st_as_sf(joined_df) # coerce to an sf object Now, we have an sf object that we can use for plotting. 3.2 Plotting spatial data To plot the number of murders by state, we can use ggplot with the geom function, geom_sf. ggplot(joined_df_sf) + geom_sf(aes(fill = Assault)) Another useful mapview(joined_df_sf[&quot;Murder&quot;]) 3.3 Assignment 3 To do this assignment, install the package spData, which contains the data you will use. Join the worldbank_df with the world sf. Take a look at what each data set contains and what is a good variable to join on. Using that variable, join the two using a left_join. With your combined data set (an sf), visualize the population growth in African countries. With your combined data set (an sf), visualize the urban population as a proportion of total population of European countries. Is there are an association between life expectancy and population growth? 3.4 ACS Data We will look at American Community Survey (ACS) data to explore time and space concepts. What is the ACS: Annual survey of 3.5 million US households Covers topics not available in decennial US Census data (e.g. income, education, language, housing characteristics) Available as 1-year estimates (for geographies of population 65,000 and greater) and 5-year estimates (for geographies down to the block group) Data delivered as estimates characterized by margins of error We will use a package called tidycensus to explore ACS data. To use tidycensus, you will need a Census API key. Visit this page to request a key, then activate the key from the link in your email. Once activated, use the census_api_key() function to set your key as an environment variable. library(tidycensus) census_api_key(&quot;YOUR KEY GOES HERE&quot;, install = TRUE) By default, ACS data are returned by get_acs() with the following five columns: GEOID: unique identifier for the Census geographic unit NAME: A descriptive name for the geographic unit (e.g. a state name) variable: the ACS variable ID estimate: The ACS estimate. Estimates are interpretated as characteristics rather than true counts. moe: The margin of error associated with the estimate at a 90 percent confidence level. Using the argument geometry = T will include a multipolygon for each row. This way, the data is an sf object on which you can use the plotting tools above. Here are some examples 3.4.1 Median age by state median_age = get_acs( geography = &quot;state&quot;, variables = &quot;B01002_001&quot;, year = 2021, survey = &quot;acs1&quot;, geometry = T ) 3.4.2 Median age within Strafford County strafford_median_age &lt;- get_acs( geography = &quot;tract&quot;, variables = &quot;B01002_001&quot;, state = &quot;NH&quot;, county = &quot;Strafford&quot;, year = 2021, geometry = T ) 3.4.3 Median home value by county median_home_value &lt;- get_acs( geography = &quot;county&quot;, variables = &quot;B25077_001&quot;, survey = &quot;acs1&quot;, year = 2019, geometry = T ) 3.5 Assignment 4 Choose a variable in the ACS1 survey that you think is interesting. See how it changes from 2005 - 2021 (by year)? Did the change(s) depend on geography? Make the best possible visualization you can for this. You can use load_variables(2021, “acs1”). "],["statistical-analysis.html", "Chapter 4 Statistical Analysis 4.1 Time series 4.2 Exercise 4.3 Spatial Data", " Chapter 4 Statistical Analysis 4.1 Time series A central concept in time series analysis is autocorrelation. This is the correlation between \\(y_t\\) and its lagged value. For a lag of \\(h\\), this is the correlation between \\(y_t\\) and \\(y_{t-h}\\). Let’s first review correlation. For two vectors of data, \\(x\\) and \\(y\\), the correlation between the two is, \\[\\begin{align} \\text{cor}(x,y) &amp;= \\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\sigma_x \\sigma_y} \\end{align}\\] set.seed(1) n = 5 x = rnorm(n) y = rnorm(n) # correlation sum((x - mean(x)) * (y - mean(y)) ) / ((n-1)*sd(x) * sd(y)) # 0.2789049 cor(x, y) # 0.2789049 The autocorrelation takes this concept to a single time series. Autocorrelation, sometimes known as serial correlation in the discrete time case, is the correlation of a signal with a delayed copy of itself as a function of delay. The autocorrelation for a time series \\(y\\) at lag \\(k\\) is: \\[\\begin{equation} r_k = \\frac{ \\sum_{t = k + 1}^{T} (y_t - \\bar{y})(y_{t-k} - \\bar{y}) }{\\sum_{t=1}^T (y_t - \\bar{y})^2 } \\end{equation}\\] Here is a simple example of computing a lag 1 autocorrelation. a = c(1,2,3,4,5) a1 = c(1,2,3,4) a2 = c(2,3,4,5) # lag 1 autocorrelation sum( (a1 - mean(a)) * (a2 - mean(a))) / (sum( (a - mean(a))^2 ) ) # by hand (acf(a)) This statistic can be found using the acf function. Let’s use the quantmod package to look at the acf of Google’s stock. goog = getSymbols(&#39;GOOG&#39;, from=&#39;2020-10-07&#39;, to=&#39;2023-02-10&#39;,auto.assign = FALSE) goog$diff = diff(goog$GOOG.Close) plot(goog$diff) acf(goog$diff[-1]) 4.2 Exercise If your team’s data has a time component(s), explore the ACF of all time series variables as well as their differenced values. What does this tell you? If your team doesn’t have time indexed data, find a different security from quandl and explore its ACF. How would you explain this? Also, look at the differenced value of the security’s close price and its ACF. Compute the lag 10 autocorrelation “by hand” and check using acf 4.3 Spatial Data Autocorrelation (whether spatial or not) is a measure of similarity (correlation) between nearby observations. In the context of time series, autocorrelation measures the similarity of observations that are nearby in time. The concept of spatial autocorrelation is an extension of this. However, there are some complications. In time, there is only a single dimension to consider when quantifying the distance between observations. For spatial data, there are many possible ways in which observations may be nearby other observations. One way to measure spatial autocorrelation is a statistic called Moran’s I. This statistic quantifies the relationship between a variable and its neighbors’ weighted average. Key steps to compute this statistic are: Get the set of neighbors for each polygon (poly2nb) Assign weights to each neighbor (nb2listw), compute a weighted mean (lag.listw) Quantify the linear relationship (lm) To illustrate, I will use ACS data on median home values for counties in New Hampshire. Let us first obtain and visualize this data. home_value &lt;- get_acs( geography = &quot;county&quot;, state = &quot;NH&quot;, variables = &quot;B25077_001&quot;, # median home value survey = &quot;acs1&quot;, year = 2019, geometry = T ) county num Strafford 1 Rockingham 2 Grafton 3 Hillsborough 4 Cheshire 5 Merrimack 6 The table above shows the mapping of counties to integers. The next step to take is to get a list of neighbors for each county. This is achieved with the function poly2nb (from the package spdep ). As an example, the neighbors of the first county are, nb &lt;- poly2nb(home_value, queen=F) nb[[1]] # neighbors of county 1 ## [1] 2 6 Next, we need to take these neighbors and give them weights (using the nb2listw function), then use these weights to compute the weighted average of neighbors for each country (using the lag.listw function). See example below, lw &lt;- nb2listw(nb, style=&quot;W&quot;) inc.lag &lt;- lag.listw(lw, home_value$estimate) moransi.df = data.frame(neighbors = inc.lag, observations = home_value$estimate) ggplot(moransi.df, aes(x= observations, y = neighbors)) + geom_point() + geom_smooth(formula = y ~ x, method = &quot;lm&quot;) + ggtitle(&quot;Morans I plot&quot;) The Moran’s I is the slope of this regression line. In this case, -0.06. This can also be computed as, lm(neighbors ~ observations, data = moransi.df) %&gt;% tidy() %&gt;% kable() term estimate std.error statistic p.value (Intercept) 2.957523e+05 4.543672e+04 6.5091039 0.0028750 observations -6.096390e-02 1.680815e-01 -0.3627043 0.7351786 This relationship is not significant. However, if we looked at all counties in the US, we would see a significant spatial relationship. home_value2 = readRDS(&quot;home_value2.RDS&quot;) # all counties data nb = poly2nb(home_value2) # neighbors list wts = nb2listw(nb, zero.policy = TRUE) # weights nbs = lag.listw(wts, home_value2$estimate) # weighted average for each county df = data.frame(neighbors = nbs, obs = home_value2$estimate) %&gt;% na.omit() # pack it up into a data frame "],["other-topics.html", "Chapter 5 Other Topics 5.1 Data Processing 5.2 Data Manipulation 5.3 Time Series", " Chapter 5 Other Topics 5.1 Data Processing To motivate this section on data processing, we will use the (fictional) data below that contains order information for a retail company. As is almost always the case, this data needs some pre-processing before it is in shape for exploration and analysis. Copy and paste the code below to obtain this data frame in your environment. The processing steps we carry out below rely on the lubridate package, so start your script with library(lubridate). set.seed(1) customer = c(&quot;113 - Shaws&quot;, &quot;217 - Shaws&quot;, &quot;2114 - WalMart&quot;, &quot;99 - WalMart&quot;, &quot;23 - CVS&quot;, &quot;09 - CVS&quot;) # numbers dash name product = c(&quot;WX1X - 9 gal - Jelly&quot;, &quot;WX1P - 4 gal - Jelly&quot;, &quot;QP1X - 11 gal - Grape Juice&quot;, &quot;QP1 - 7 gal - Fruit Juice&quot;, &quot;TYL - 1 gal - Peanut Butter&quot;, &quot;LL - 2 gal - Jam&quot;) # letters size description business_unit = c(&quot;123 Retail&quot;, &quot;437 Consumer&quot;, &quot;990 International&quot;, &quot;222 Retail&quot;, &quot;49 Consumer&quot;, &quot;09 International&quot;) # number dash name df = data.frame(customer, product, business_unit, &quot;1-1-2022&quot; = rpois(6, 1100), &quot;2-1-2022&quot; = rpois(6, 1200), check.names = F) Table 5.1: A fake dataset on product shipments. customer product business_unit 1-1-2022 2-1-2022 113 - Shaws WX1X - 9 gal - Jelly 123 Retail 1079 1225 217 - Shaws WX1P - 4 gal - Jelly 437 Consumer 1144 1219 2114 - WalMart QP1X - 11 gal - Grape Juice 990 International 1142 1189 99 - WalMart QP1 - 7 gal - Fruit Juice 222 Retail 1113 1172 23 - CVS TYL - 1 gal - Peanut Butter 49 Consumer 1048 1178 09 - CVS LL - 2 gal - Jam 09 International 1116 1189 Our pre-processing of this particular dataset consists primarily of cleaning up the character (string) variables and dealing with dates. 5.1.1 Cleaning up strings For a reference on the use of strings in R, see this excellent resource. For the purpose of our analysis, some aspects of this dataset that are distracting for analysis include: The meaningless (to us) numbers that precede the customer name and business unit. The meaningless letters that precede the product description. The product variable includes the size which would ideally exist in its own column (i.e., we want a size column whose first entry is 9) Let’s work through each of these using the functions str_sub, str_locate, and str_split. Let’s first go over what these functions do and how they can be used for this task. str_sub(&quot;The big short&quot;, start = 5, end = 7) # big # the input can also be a vector x &lt;- c(&quot;Apple&quot;, &quot;Banana&quot;, &quot;Pear&quot;) # character vector str_sub(x, 1, 3) # &quot;App&quot; &quot;Ban&quot; &quot;Pea&quot; str_locate(&quot;This&quot;, &quot;s&quot;) # gives the starting and ending position of the &#39;s&#39; in &#39;This&#39; How can these two functions (str_sub, str_locate) be used in conjunction to deal with the first issue identified above? Try this out using the first entry from customer. The patterns that are located can be much more general than a literal letter. These patters are known as regular expressions. For example, suppose we would like to locate the part of the string that consists of the letter a followed by any number. The regular expression that represents any digit is \\d. str_locate(&quot;aardvark a3&quot;, &quot;a\\\\d&quot;) # 10 11 For more on matching regular expressions in R, see this document on the topic. Next, let’s deal with the product variable. This variable contains two potentially useful pieces of information and one useless piece. They are separated by a dash. This presents a good use case for the str_split function. Here is an example where we have two data points that are separated by a colon and we would like to extract them. str_split(&quot;200:300&quot;, &quot;:&quot;, simplify = T) # 200 300 What should we split the product variable on? This will give us a matrix, with two useful columns. We want to grab these columns and include them in our data frame as size and product_name. Refresher on how to carry out these sorts of tasks - How R thinks about data. 5.1.2 Assignment 1 Use the functions described above (str_sub, str_locate, and str_split) to obtain the following cleaned up data frame. customer product business_unit 1-1-2022 2-1-2022 size Shaws Jelly Retail 1079 1225 9 gal Shaws Jelly Consumer 1144 1219 4 gal WalMart Grape Juice International 1142 1189 11 gal WalMart Fruit Juice Retail 1113 1172 7 gal CVS Peanut Butter Consumer 1048 1178 1 gal CVS Jam International 1116 1189 2 gal Dealing with dates (part 1). The output from part one gets us pretty close to our desired analytic dataset. However, it turns out that for our analysis we would like the data to be in long format, where each row contains information on a single month. So, the first row of the dataset would have the January 2022 units of 9 gal Jelly shipped to Shaw’s. This is achieved using the function pivot_longer. Let’s take a look at the example from the documentation to figure out what parameters to use to achieve this Once you figure out the appropriate arguments for pivot_longer, the head of your new data frame should look like this: customer product business_unit size month units Shaws Jelly Retail 9 gal 1-1-2022 1079 Shaws Jelly Retail 9 gal 2-1-2022 1225 Shaws Jelly Consumer 4 gal 1-1-2022 1144 Shaws Jelly Consumer 4 gal 2-1-2022 1219 WalMart Grape Juice International 11 gal 1-1-2022 1142 WalMart Grape Juice International 11 gal 2-1-2022 1189 Dealing with dates (part 2). Finally, we want R to recognize our month variable as a date (rather than a character). A helpful package for dates is lubridate. Download and load this package into your workspace and use the function mdy on the month variable to coerce this variable to become a date. The str of your data frame should now look like this: ## tibble [12 × 6] (S3: tbl_df/tbl/data.frame) ## $ customer : chr [1:12] &quot;Shaws&quot; &quot;Shaws&quot; &quot;Shaws&quot; &quot;Shaws&quot; ... ## $ product : chr [1:12] &quot;Jelly&quot; &quot;Jelly&quot; &quot;Jelly&quot; &quot;Jelly&quot; ... ## $ business_unit: chr [1:12] &quot;Retail&quot; &quot;Retail&quot; &quot;Consumer&quot; &quot;Consumer&quot; ... ## $ size : chr [1:12] &quot;9 gal&quot; &quot;9 gal&quot; &quot;4 gal&quot; &quot;4 gal&quot; ... ## $ month : Date[1:12], format: &quot;2022-01-01&quot; &quot;2022-02-01&quot; ... ## $ units : int [1:12] 1079 1225 1144 1219 1142 1189 1113 1172 1048 1178 ... Notice how the month variable has format Date. Your data frame is now ready for exploration and analysis! 5.2 Data Manipulation We will discuss data transformation using data for all flights that departed NYC (i.e. JFK, LGA or EWR) in 2013. This data is contained in the flights data frame that comes from the nycflights13 package. Before beginning, install this package and load it (using nycflights13). There are five key dplyr functions that allow you to solve the vast majority of your data manipulation challenges: Pick observations by their values (filter()). Reorder the rows (arrange()). Pick variables by their names (select()). Create new variables with functions of existing variables (mutate()). Collapse many values down to a single summary (summarise()). This is often used in conjunction with group_by(), which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. Let’s go through a quick use case for each of these. Further detail can be found here. 5.2.1 Filter Suppose we want to look at just morning flights (before 10am). morning_df = filter(flights, hour &lt; 10) How about morning flights in January. morning_df = filter(flights, hour &lt; 10, month == 1) 5.2.2 Arrange What are the attributes of the flights with the biggest departure delays? arrange(flights, desc(dep_delay)) 5.2.3 Select Suppose we don’t care about the flight number. arrange(flights, !flight) 5.2.4 Mutate Suppose we want to add a speed variable. mutate(flights, speed = distance/air_time * 60) 5.2.5 Group-By/ Summarise Suppose we want to know the mean departure delay by airline. flights %&gt;% group_by(carrier) %&gt;% summarize(mean_delay = mean(dep_delay, na.rm = T), n = n()) %&gt;% arrange(desc(mean_delay)) 5.2.6 Assignment 2 How many flights … Had an arrival delay of two or more hours Flew to Houston (IAH or HOU) Were operated by United, American, or Delta Departed in summer (July, August, and September) Arrived more than two hours late, but didn’t leave late Were delayed by at least an hour, but made up over 30 minutes in flight Add two new variables to flights that convert dep_time and arr_time to a more convenient representation of number of minutes since midnight. Look at the number of canceled flights per day. Is there a pattern over the course of the year? How about the proportion of canceled flights over the course of the year? What time of day should you fly if you want to avoid delays as much as possible? 5.3 Time Series 5.3.1 Decomposition We want to decompose our time series into three parts: a trend component (\\(T\\)), a seasonality component (\\(S\\)), and a random component (\\(R\\)). That is, for each observation \\(Y_t\\), we want to break it down into three parts: \\(Y_T = T_t + S_t + R_t\\) First, let’s talk about the trend component (T). The trend component is a moving average, which you can obtain using the ma function within the forecast library. set.seed(1) # generate some fake data that resembles our real dataset (noce the week_no variable, which # corresponds with the enterprise week number from your original data) trend = seq( from = 10, to = 10 + 52 * 3 - 1, by = 1) df = data.frame( invoiced = trend + (rnorm(52 * 3, mean = 25, sd = 40)), week_no = rep(1:52, 3), # week number id = 1:(52 * 3)) plot(df$invoiced, type = &quot;l&quot;) y_ts = ts(df$invoiced, frequency = 52) # make your data into a time series object df$trend = ma(y_ts, order = 52) # use the ma function from the forecast library Now that we have added a trend variable to our data frame, let’s get the seasonal (\\(S\\)) component. To estimate the seasonal component for each week, simply average the detrended values for that week. These seasonal component values are then adjusted to ensure that they add to zero. # 1. subtract the trend df$detrend = df$invoiced - df$trend # 2. group by week number and take the average of the de-trended values df = df %&gt;% group_by(week_no) %&gt;% mutate(S1 = mean(detrend, na.rm = T)) %&gt;% ungroup() %&gt;% mutate(S = S1 - mean(S1)) %&gt;% # make sure the sum of the seasonal components is 0 select(!c(detrend, S1)) Finally, the random component is calculated by subtracting the estimated seasonal and trend-cycle components. That is, \\(R_t = Y_t - T_t - S_t\\). df = mutate(df, R = invoiced - trend - S) Now, plot each line: df %&gt;% pivot_longer(!(c(&quot;week_no&quot;, &quot;id&quot;))) %&gt;% ggplot(aes(id, value, color = name)) + geom_line() + theme_minimal() + ggtitle(&quot;Decomposition of invoiced time series&quot;) 5.3.2 Forecasting Let’s consider some very simple potential forecast models. The simplest potential model is to forecast future values as the last observed value. That is, \\[\\begin{equation} \\hat{y}_{T+1 | T} = y_T \\end{equation}\\] Another very simple model consists of forecasting future values as the average over the entire observed series. That is, \\[\\begin{equation} \\hat{y}_{T+1 | T} = \\frac{1}{T} \\sum_{i=1}^T y_i \\end{equation}\\] A slightly more complicated approach that typically works better is to forecast future values as a weighted average of past values, with higher weights assigned to more recent observations. This model can be expressed as, \\[\\begin{equation} \\hat{y}_{T+h | T} = \\alpha y_T + (1-\\alpha) \\hat{y}_{T | T - 1} \\end{equation}\\] How can we show that weights are decreasing in time? What does this expression imply about forecasts from this model? This is implemented in ses below. The next level up in complexity is using the same idea but adding a trend estimate. In this case, the forecast values are, \\[\\begin{equation} \\hat{y}_{T+h | T} = l_T + h b_T, \\end{equation}\\] Where \\(l\\) is the estimated level of the time series and \\(b_T\\) is the estimated trend at time \\(T\\). What does this expression imply about forecasts from this model? This is implemented in holt below. fx1 &lt;- ses(y_ts, h=5) fx2 &lt;- holt(y_ts, h=15) round(accuracy(fx1),2) ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 6.46 37.15 28.71 -21.53 55.35 0.56 -0.08 autoplot(y_ts) + autolayer(fx1, series=&quot;Simple Exponential Smoothing&quot;, PI=F) + autolayer(fx2, series=&quot;Holt Winters&quot;, PI=F) + ylab(&quot;Invoiced&quot;) + xlab(&quot;Year&quot;) 5.3.3 Statistical model One of the drawbacks of the forecasting methods we have used so far is that there is no uncertainty quantification. Fortunately, there is an easy way to formulate the exponential smoothing models we have been working with as a statistical model. For more detail, see this excellent resource. mod = ets(y_ts, model=&quot;ZZZ&quot;, damped=NULL, alpha=NULL, beta=NULL, gamma=NULL, phi=NULL, lambda=NULL, biasadj=FALSE, additive.only=FALSE, restrict=TRUE, allow.multiplicative.trend=FALSE) Once the model is fit, we can see which model was chosen using the summary function. We label each state space model as ETS( \\(\\cdot, \\cdot, \\cdot\\)), for (Error, Trend, Seasonal). The possibilities for each component are: Error = {\\(A,M\\)}, Trend = {\\(N,A,A_d\\)}, and Seasonal = {\\(N,A,M\\)}. Here, \\(M\\) denotes multiply and \\(A\\) denotes add. In an \\(M\\) type model, the component multiplies (i.e., a season in which invoices are 1.1 times as large). ## ETS(A,A,N) ## ## Call: ## ets(y = y_ts, model = &quot;ZZZ&quot;, damped = NULL, alpha = NULL, beta = NULL, ## ## Call: ## gamma = NULL, phi = NULL, additive.only = FALSE, lambda = NULL, ## ## Call: ## biasadj = FALSE, restrict = TRUE, allow.multiplicative.trend = FALSE) ## ## Smoothing parameters: ## alpha = 0.002 ## beta = 0.002 ## ## Initial states: ## l = 30.6217 ## b = 1.054 ## ## sigma: 35.968 ## ## AIC AICc BIC ## 1911.506 1911.906 1926.755 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -2.913354 35.50392 28.2599 -29.14288 55.95159 0.552141 -0.03601155 There is also a straightforward way to forecast using the fitted model. mod %&gt;% forecast(h=15) %&gt;% autoplot() To do Cut your time series into a training set (all but the last 3 months of data) and a test set (last 3 months) Fit a state-space model using ETS to your (weekly) time series. What model was chosen? Compare forecasts for 12 weeks ahead to the actual data. Now make your training set and test set into monthly data and repeat step 2. On a monthly basis, how do the residuals of the weekly model compare to the residuals of the monthly model? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
